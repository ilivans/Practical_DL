{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 50% (50% points)\n",
    "    * 60% (60% points)\n",
    "    * 65% (70% points)\n",
    "    * 70% (80% points)\n",
    "    * 75% (90% points)\n",
    "    * 80% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
    "* you __can__ use validation data for training, but you __can't'__ do anything with test data apart from running the evaluation procedure.\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
    "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root=path_to_cifar_like_in_seminar, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found. Downloading...\n",
      "(40000, 3, 32, 32) (40000,)\n"
     ]
    }
   ],
   "source": [
    "from cifar import load_cifar10\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_cifar10(\"cifar_data\")\n",
    "class_names = np.array(['airplane','automobile ','bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck'])\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module(\"conv1\", nn.Conv2d(3, 32, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv1_bn\", nn.BatchNorm2d(32))\n",
    "model.add_module(\"conv1_relu\", nn.ReLU())\n",
    "model.add_module(\"pool1\", nn.MaxPool2d(2))\n",
    "model.add_module(\"dp1\", nn.Dropout2d(0.1))\n",
    "\n",
    "model.add_module(\"conv2\", nn.Conv2d(32, 64, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv2_bn\", nn.BatchNorm2d(64))\n",
    "model.add_module(\"conv2_relu\", nn.ReLU())\n",
    "model.add_module(\"pool2\", nn.MaxPool2d(2))\n",
    "model.add_module(\"dp2\", nn.Dropout2d(0.1))\n",
    "\n",
    "model.add_module(\"conv3\", nn.Conv2d(64, 128, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv3_bn\", nn.BatchNorm2d(128))\n",
    "model.add_module(\"conv3_relu\", nn.ReLU())\n",
    "model.add_module(\"pool3\", nn.MaxPool2d(2))\n",
    "model.add_module(\"dp3\", nn.Dropout2d(0.1))\n",
    "\n",
    "model.add_module(\"conv4\", nn.Conv2d(128, 256, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv4_bn\", nn.BatchNorm2d(256))\n",
    "model.add_module(\"conv4_relu\", nn.ReLU())\n",
    "model.add_module(\"pool4\", nn.MaxPool2d(2))\n",
    "model.add_module(\"dp4\", nn.Dropout2d(0.1))\n",
    "\n",
    "model.add_module(\"flatten\", Flatten())\n",
    "\n",
    "model.add_module(\"dense1_logits\", nn.Linear(2*2*256, 128))\n",
    "model.add_module(\"dense1_bn\", nn.BatchNorm1d(128))\n",
    "model.add_module(\"dense1_relu\", nn.ReLU())\n",
    "model.add_module(\"dense1_dp\", nn.Dropout(0.6))\n",
    "\n",
    "model.add_module(\"dense2_logits\", nn.Linear(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch))\n",
    "    y_batch = Variable(torch.LongTensor(y_batch))\n",
    "    logits = model(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Training __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53d9112cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "means = np.array((0.4914, 0.4822, 0.4465))\n",
    "stds = np.array((0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_augment = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation([-30, 30]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "aug = transform_augment((np.transpose(X_train[0],(1, 2, 0)) * 255).astype(np.uint8)).numpy()\n",
    "plt.imshow(((np.transpose(aug, (1,2,0)) * stds + means)* 255).astype(np.uint8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batchsize):\n",
    "    indices = np.random.permutation(np.arange(len(X)))\n",
    "    for start in range(0, len(indices), batchsize):\n",
    "        ix = indices[start: start + batchsize]\n",
    "        yield X[ix], y[ix]\n",
    "        \n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 113.205s\n",
      "  training loss (in-iteration): \t1.783646\n",
      "  validation accuracy: \t\t\t49.56 %\n",
      "Epoch 2 of 100 took 121.702s\n",
      "  training loss (in-iteration): \t1.535484\n",
      "  validation accuracy: \t\t\t55.02 %\n",
      "Epoch 3 of 100 took 118.269s\n",
      "  training loss (in-iteration): \t1.422615\n",
      "  validation accuracy: \t\t\t60.79 %\n",
      "Epoch 4 of 100 took 124.361s\n",
      "  training loss (in-iteration): \t1.351170\n",
      "  validation accuracy: \t\t\t62.51 %\n",
      "Epoch 5 of 100 took 115.373s\n",
      "  training loss (in-iteration): \t1.280668\n",
      "  validation accuracy: \t\t\t64.83 %\n",
      "Epoch 6 of 100 took 116.907s\n",
      "  training loss (in-iteration): \t1.238560\n",
      "  validation accuracy: \t\t\t66.77 %\n",
      "Epoch 7 of 100 took 109.538s\n",
      "  training loss (in-iteration): \t1.203813\n",
      "  validation accuracy: \t\t\t68.08 %\n",
      "Epoch 8 of 100 took 109.480s\n",
      "  training loss (in-iteration): \t1.172176\n",
      "  validation accuracy: \t\t\t70.42 %\n",
      "Epoch 9 of 100 took 117.892s\n",
      "  training loss (in-iteration): \t1.137985\n",
      "  validation accuracy: \t\t\t69.98 %\n",
      "Epoch 10 of 100 took 117.855s\n",
      "  training loss (in-iteration): \t1.117702\n",
      "  validation accuracy: \t\t\t70.10 %\n",
      "Epoch 11 of 100 took 119.381s\n",
      "  training loss (in-iteration): \t1.102473\n",
      "  validation accuracy: \t\t\t71.00 %\n",
      "Epoch 12 of 100 took 117.797s\n",
      "  training loss (in-iteration): \t1.082053\n",
      "  validation accuracy: \t\t\t72.07 %\n",
      "Epoch 13 of 100 took 122.534s\n",
      "  training loss (in-iteration): \t1.057039\n",
      "  validation accuracy: \t\t\t72.62 %\n",
      "Epoch 14 of 100 took 114.998s\n",
      "  training loss (in-iteration): \t1.044459\n",
      "  validation accuracy: \t\t\t72.93 %\n",
      "Epoch 15 of 100 took 108.937s\n",
      "  training loss (in-iteration): \t1.029575\n",
      "  validation accuracy: \t\t\t74.18 %\n",
      "Epoch 16 of 100 took 106.920s\n",
      "  training loss (in-iteration): \t1.016932\n",
      "  validation accuracy: \t\t\t74.50 %\n",
      "Epoch 17 of 100 took 107.954s\n",
      "  training loss (in-iteration): \t1.007674\n",
      "  validation accuracy: \t\t\t74.20 %\n",
      "Epoch 18 of 100 took 110.159s\n",
      "  training loss (in-iteration): \t0.992762\n",
      "  validation accuracy: \t\t\t75.67 %\n",
      "Epoch 19 of 100 took 108.065s\n",
      "  training loss (in-iteration): \t0.987248\n",
      "  validation accuracy: \t\t\t74.99 %\n",
      "Epoch 20 of 100 took 108.214s\n",
      "  training loss (in-iteration): \t0.973884\n",
      "  validation accuracy: \t\t\t76.28 %\n",
      "Epoch 21 of 100 took 106.698s\n",
      "  training loss (in-iteration): \t0.958729\n",
      "  validation accuracy: \t\t\t75.84 %\n",
      "Epoch 22 of 100 took 107.543s\n",
      "  training loss (in-iteration): \t0.954085\n",
      "  validation accuracy: \t\t\t76.25 %\n",
      "Epoch 23 of 100 took 106.558s\n",
      "  training loss (in-iteration): \t0.953100\n",
      "  validation accuracy: \t\t\t77.14 %\n",
      "Epoch 24 of 100 took 107.128s\n",
      "  training loss (in-iteration): \t0.938266\n",
      "  validation accuracy: \t\t\t77.79 %\n",
      "Epoch 25 of 100 took 106.681s\n",
      "  training loss (in-iteration): \t0.936441\n",
      "  validation accuracy: \t\t\t76.83 %\n",
      "Epoch 26 of 100 took 107.190s\n",
      "  training loss (in-iteration): \t0.922974\n",
      "  validation accuracy: \t\t\t77.58 %\n",
      "Epoch 27 of 100 took 107.082s\n",
      "  training loss (in-iteration): \t0.919692\n",
      "  validation accuracy: \t\t\t77.62 %\n",
      "Epoch 28 of 100 took 106.083s\n",
      "  training loss (in-iteration): \t0.914722\n",
      "  validation accuracy: \t\t\t77.85 %\n",
      "Epoch 29 of 100 took 107.534s\n",
      "  training loss (in-iteration): \t0.906028\n",
      "  validation accuracy: \t\t\t77.73 %\n",
      "Epoch 30 of 100 took 107.373s\n",
      "  training loss (in-iteration): \t0.898188\n",
      "  validation accuracy: \t\t\t77.94 %\n",
      "Epoch 31 of 100 took 106.494s\n",
      "  training loss (in-iteration): \t0.897289\n",
      "  validation accuracy: \t\t\t78.38 %\n",
      "Epoch 32 of 100 took 107.478s\n",
      "  training loss (in-iteration): \t0.890039\n",
      "  validation accuracy: \t\t\t78.83 %\n",
      "Epoch 33 of 100 took 106.564s\n",
      "  training loss (in-iteration): \t0.889464\n",
      "  validation accuracy: \t\t\t79.02 %\n",
      "Epoch 34 of 100 took 107.657s\n",
      "  training loss (in-iteration): \t0.880494\n",
      "  validation accuracy: \t\t\t79.45 %\n",
      "Epoch 35 of 100 took 107.078s\n",
      "  training loss (in-iteration): \t0.876701\n",
      "  validation accuracy: \t\t\t78.49 %\n",
      "Epoch 36 of 100 took 108.278s\n",
      "  training loss (in-iteration): \t0.876845\n",
      "  validation accuracy: \t\t\t78.73 %\n",
      "Epoch 37 of 100 took 106.483s\n",
      "  training loss (in-iteration): \t0.867714\n",
      "  validation accuracy: \t\t\t78.01 %\n",
      "Epoch 38 of 100 took 106.806s\n",
      "  training loss (in-iteration): \t0.861808\n",
      "  validation accuracy: \t\t\t79.35 %\n",
      "Epoch 39 of 100 took 107.697s\n",
      "  training loss (in-iteration): \t0.859724\n",
      "  validation accuracy: \t\t\t79.67 %\n",
      "Epoch 40 of 100 took 108.674s\n",
      "  training loss (in-iteration): \t0.858774\n",
      "  validation accuracy: \t\t\t79.89 %\n",
      "Epoch 41 of 100 took 107.181s\n",
      "  training loss (in-iteration): \t0.847271\n",
      "  validation accuracy: \t\t\t79.61 %\n",
      "Epoch 42 of 100 took 107.090s\n",
      "  training loss (in-iteration): \t0.841856\n",
      "  validation accuracy: \t\t\t79.64 %\n",
      "Epoch 43 of 100 took 108.233s\n",
      "  training loss (in-iteration): \t0.849646\n",
      "  validation accuracy: \t\t\t79.96 %\n",
      "Epoch 44 of 100 took 106.690s\n",
      "  training loss (in-iteration): \t0.841521\n",
      "  validation accuracy: \t\t\t80.27 %\n",
      "Epoch 45 of 100 took 108.390s\n",
      "  training loss (in-iteration): \t0.843386\n",
      "  validation accuracy: \t\t\t80.41 %\n",
      "Epoch 46 of 100 took 107.734s\n",
      "  training loss (in-iteration): \t0.838901\n",
      "  validation accuracy: \t\t\t80.33 %\n",
      "Epoch 47 of 100 took 108.025s\n",
      "  training loss (in-iteration): \t0.840591\n",
      "  validation accuracy: \t\t\t80.09 %\n",
      "Epoch 48 of 100 took 107.960s\n",
      "  training loss (in-iteration): \t0.827102\n",
      "  validation accuracy: \t\t\t80.34 %\n",
      "Epoch 49 of 100 took 106.595s\n",
      "  training loss (in-iteration): \t0.828661\n",
      "  validation accuracy: \t\t\t79.40 %\n",
      "Epoch 50 of 100 took 107.111s\n",
      "  training loss (in-iteration): \t0.821002\n",
      "  validation accuracy: \t\t\t79.97 %\n",
      "Epoch 51 of 100 took 105.901s\n",
      "  training loss (in-iteration): \t0.814484\n",
      "  validation accuracy: \t\t\t80.68 %\n",
      "Epoch 52 of 100 took 106.694s\n",
      "  training loss (in-iteration): \t0.819556\n",
      "  validation accuracy: \t\t\t80.77 %\n",
      "Epoch 53 of 100 took 105.970s\n",
      "  training loss (in-iteration): \t0.821251\n",
      "  validation accuracy: \t\t\t80.76 %\n",
      "Epoch 54 of 100 took 107.911s\n",
      "  training loss (in-iteration): \t0.808443\n",
      "  validation accuracy: \t\t\t80.93 %\n",
      "Epoch 55 of 100 took 107.271s\n",
      "  training loss (in-iteration): \t0.812194\n",
      "  validation accuracy: \t\t\t80.98 %\n",
      "Epoch 56 of 100 took 107.152s\n",
      "  training loss (in-iteration): \t0.799379\n",
      "  validation accuracy: \t\t\t81.06 %\n",
      "Epoch 57 of 100 took 107.639s\n",
      "  training loss (in-iteration): \t0.805013\n",
      "  validation accuracy: \t\t\t81.10 %\n",
      "Epoch 58 of 100 took 107.582s\n",
      "  training loss (in-iteration): \t0.800122\n",
      "  validation accuracy: \t\t\t81.23 %\n",
      "Epoch 59 of 100 took 106.779s\n",
      "  training loss (in-iteration): \t0.805818\n",
      "  validation accuracy: \t\t\t81.71 %\n",
      "Epoch 60 of 100 took 108.061s\n",
      "  training loss (in-iteration): \t0.800627\n",
      "  validation accuracy: \t\t\t81.27 %\n",
      "Epoch 61 of 100 took 107.605s\n",
      "  training loss (in-iteration): \t0.795440\n",
      "  validation accuracy: \t\t\t81.36 %\n",
      "Epoch 62 of 100 took 106.583s\n",
      "  training loss (in-iteration): \t0.801672\n",
      "  validation accuracy: \t\t\t81.16 %\n",
      "Epoch 63 of 100 took 106.550s\n",
      "  training loss (in-iteration): \t0.789364\n",
      "  validation accuracy: \t\t\t81.68 %\n",
      "Epoch 64 of 100 took 107.571s\n",
      "  training loss (in-iteration): \t0.785443\n",
      "  validation accuracy: \t\t\t81.34 %\n",
      "Epoch 65 of 100 took 113.279s\n",
      "  training loss (in-iteration): \t0.784687\n",
      "  validation accuracy: \t\t\t81.91 %\n",
      "Epoch 66 of 100 took 109.541s\n",
      "  training loss (in-iteration): \t0.785956\n",
      "  validation accuracy: \t\t\t81.83 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-d33ff11a3aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransform_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# train on batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-07451d0a3d2e>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(X_batch, y_batch)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.5/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    141\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    142\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmax_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 100 # total amount of full passes over training data\n",
    "batch_size = 64  # number of samples processed in one SGD iteration\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        X_batch = np.array([transform_augment((np.transpose(x,(1, 2, 0)) * 255).astype(np.uint8)).numpy() for x in X_batch])\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.data.numpy()[0])\n",
    "        \n",
    "    # And a full pass over the validation data:\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        X_batch = np.array([transform_test((np.transpose(x,(1, 2, 0)) * 255).astype(np.uint8)).numpy() for x in X_batch])\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)))\n",
    "        y_pred = logits.max(1)[1].data.numpy()\n",
    "        val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(X_train) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(X_val) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t81.54 %\n",
      "Achievement unlocked: 110lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    X_batch = np.array([transform_test((np.transpose(x,(1, 2, 0)) * 255).astype(np.uint8)).numpy() for x in X_batch])\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)))\n",
    "    y_pred = logits.max(1)[1].data.numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 95:\n",
    "    print(\"Double-check, than consider applying for NIPS'17. SRSly.\")\n",
    "elif test_accuracy * 100 > 90:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 80:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 70:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 60:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the points below I mention validation accuracy\n",
    "\n",
    "The optimization method was Adam(lr=1e-3) each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 70%\n",
    "\n",
    "Start with 2 convolutional layers (followed by BN, ReLU and MaxPooling) and dense one"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module(\"conv1\", nn.Conv2d(3, 32, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv1_bn\", nn.BatchNorm2d(32))\n",
    "model.add_module(\"conv1_relu\", nn.ReLU())\n",
    "model.add_module(\"pool1\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"conv2\", nn.Conv2d(32, 64, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv2_bn\", nn.BatchNorm2d(64))\n",
    "model.add_module(\"conv2_relu\", nn.ReLU())\n",
    "model.add_module(\"pool2\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"flatten\", Flatten())\n",
    "\n",
    "model.add_module(\"dense1_logits\", nn.Linear(8*8*64, 10))\n",
    "\n",
    "# batch size 64\n",
    "# ~15 epochs to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 74% - added one dense layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module(\"conv1\", nn.Conv2d(3, 32, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv1_bn\", nn.BatchNorm2d(32))\n",
    "model.add_module(\"conv1_relu\", nn.ReLU())\n",
    "model.add_module(\"pool1\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"conv2\", nn.Conv2d(32, 64, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv2_bn\", nn.BatchNorm2d(64))\n",
    "model.add_module(\"conv2_relu\", nn.ReLU())\n",
    "model.add_module(\"pool2\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"flatten\", Flatten())\n",
    "\n",
    "model.add_module(\"dense1_logits\", nn.Linear(8*8*64, 128))\n",
    "model.add_module(\"dense1_bn\", nn.BatchNorm1d(128))\n",
    "model.add_module(\"dense1_relu\", nn.ReLU())\n",
    "model.add_module(\"dense1_dp\", nn.Dropout(0.5))\n",
    "\n",
    "model.add_module(\"dense2_logits\", nn.Linear(128, 10))\n",
    "\n",
    "# batch size 64\n",
    "# 15 epochs to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  77% - added one CNN layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module(\"conv1\", nn.Conv2d(3, 32, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv1_bn\", nn.BatchNorm2d(32))\n",
    "model.add_module(\"conv1_relu\", nn.ReLU())\n",
    "model.add_module(\"pool1\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"conv2\", nn.Conv2d(32, 64, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv2_bn\", nn.BatchNorm2d(64))\n",
    "model.add_module(\"conv2_relu\", nn.ReLU())\n",
    "model.add_module(\"pool2\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"conv3\", nn.Conv2d(64, 128, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv3_bn\", nn.BatchNorm2d(128))\n",
    "model.add_module(\"conv3_relu\", nn.ReLU())\n",
    "model.add_module(\"pool3\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"flatten\", Flatten())\n",
    "\n",
    "model.add_module(\"dense1_logits\", nn.Linear(4*4*128, 128))\n",
    "model.add_module(\"dense1_bn\", nn.BatchNorm1d(128))\n",
    "model.add_module(\"dense1_relu\", nn.ReLU())\n",
    "model.add_module(\"dense1_dp\", nn.Dropout(0.5))\n",
    "\n",
    "model.add_module(\"dense2_logits\", nn.Linear(128, 10))\n",
    "\n",
    "# batch size 64\n",
    "# 15 epochs to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 75% (rollback) - increased batch size to 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 77% - added another CNN layer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module(\"conv1\", nn.Conv2d(3, 32, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv1_bn\", nn.BatchNorm2d(32))\n",
    "model.add_module(\"conv1_relu\", nn.ReLU())\n",
    "model.add_module(\"pool1\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"conv2\", nn.Conv2d(32, 64, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv2_bn\", nn.BatchNorm2d(64))\n",
    "model.add_module(\"conv2_relu\", nn.ReLU())\n",
    "model.add_module(\"pool2\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"conv3\", nn.Conv2d(64, 128, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv3_bn\", nn.BatchNorm2d(128))\n",
    "model.add_module(\"conv3_relu\", nn.ReLU())\n",
    "model.add_module(\"pool3\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"conv4\", nn.Conv2d(128, 256, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv4_bn\", nn.BatchNorm2d(256))\n",
    "model.add_module(\"conv4_relu\", nn.ReLU())\n",
    "model.add_module(\"pool4\", nn.MaxPool2d(2))\n",
    "\n",
    "model.add_module(\"flatten\", Flatten())\n",
    "\n",
    "model.add_module(\"dense1_logits\", nn.Linear(2*2*128, 128))\n",
    "model.add_module(\"dense1_bn\", nn.BatchNorm1d(128))\n",
    "model.add_module(\"dense1_relu\", nn.ReLU())\n",
    "model.add_module(\"dense1_dp\", nn.Dropout(0.5))\n",
    "\n",
    "model.add_module(\"dense2_logits\", nn.Linear(128, 10))\n",
    "\n",
    "# batch size 64\n",
    "# 15 epochs to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 78% - increased dropout after dense layer to 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 80% - added 0.2 dropout after each pooling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = nn.Sequential()\n",
    "\n",
    "model.add_module(\"conv1\", nn.Conv2d(3, 32, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv1_bn\", nn.BatchNorm2d(32))\n",
    "model.add_module(\"conv1_relu\", nn.ReLU())\n",
    "model.add_module(\"pool1\", nn.MaxPool2d(2))\n",
    "model.add_module(\"dp1\", nn.Dropout2d(0.2))\n",
    "\n",
    "model.add_module(\"conv2\", nn.Conv2d(32, 64, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv2_bn\", nn.BatchNorm2d(64))\n",
    "model.add_module(\"conv2_relu\", nn.ReLU())\n",
    "model.add_module(\"pool2\", nn.MaxPool2d(2))\n",
    "model.add_module(\"dp2\", nn.Dropout2d(0.2))\n",
    "\n",
    "model.add_module(\"conv3\", nn.Conv2d(64, 128, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv3_bn\", nn.BatchNorm2d(128))\n",
    "model.add_module(\"conv3_relu\", nn.ReLU())\n",
    "model.add_module(\"pool3\", nn.MaxPool2d(2))\n",
    "model.add_module(\"dp3\", nn.Dropout2d(0.2))\n",
    "\n",
    "model.add_module(\"conv4\", nn.Conv2d(128, 256, kernel_size=3, padding=1))\n",
    "model.add_module(\"conv4_bn\", nn.BatchNorm2d(256))\n",
    "model.add_module(\"conv4_relu\", nn.ReLU())\n",
    "model.add_module(\"pool4\", nn.MaxPool2d(2))\n",
    "model.add_module(\"dp4\", nn.Dropout2d(0.2))\n",
    "\n",
    "model.add_module(\"flatten\", Flatten())\n",
    "\n",
    "model.add_module(\"dense1_logits\", nn.Linear(2*2*256, 128))\n",
    "model.add_module(\"dense1_bn\", nn.BatchNorm1d(128))\n",
    "model.add_module(\"dense1_relu\", nn.ReLU())\n",
    "model.add_module(\"dense1_dp\", nn.Dropout(0.7))\n",
    "\n",
    "model.add_module(\"dense2_logits\", nn.Linear(128, 10))\n",
    "\n",
    "# batch size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  ? - used training time augmentation\n",
    "\n",
    "too slow convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 81% - used training time augmentation + decreased all dropouts by 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model on training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train accuracy:\t\t85.21 %\n"
     ]
    }
   ],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for X_batch, y_batch in iterate_minibatches(X_train, y_train, 500):\n",
    "    X_batch = np.array([transform_test((np.transpose(x,(1, 2, 0)) * 255).astype(np.uint8)).numpy() for x in X_batch])\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)))\n",
    "    y_pred = logits.max(1)[1].data.numpy()\n",
    "    test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)\n",
    "    \n",
    "print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, my name is `Ilya Ivanov`, and here's my story\n",
    "\n",
    "A long time ago in a galaxy far far away, when it was still more than an hour before the deadline, i got an idea:\n",
    "\n",
    "##### I gonna build a neural network, that\n",
    "* predicts the class of an image\n",
    "* belongs to CNN architectures family\n",
    "* because this was the task\n",
    "\n",
    "How could i be so naive?!\n",
    "\n",
    "##### One day, with no signs of warning,\n",
    "This thing has finally converged and\n",
    "* achieved 81.5% test accuracy\n",
    "* all worked except big batch size\n",
    "\n",
    "\n",
    "Each of the following enhancements brought several percent of validation accuracy:\n",
    "* additional dense layer\n",
    "* additional convolutional layers\n",
    "* dropout after pooling layers\n",
    "* training time data augmentation\n",
    "\n",
    "##### Finally, after 9  iterations\n",
    "* the final architecture is above in the code (4 CNN layers + 2 Dense with BN, MaxPooling, Dropout)\n",
    "* training time data augmentation is used\n",
    "* opt. method is Adam with default learning rate (1e-3)\n",
    "\n",
    "That, having wasted 2 hours of my life training, got\n",
    "\n",
    "* accuracy on training: 85.21 %\n",
    "* accuracy on validation: 81.83 %\n",
    "* accuracy on test: 81.54 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
